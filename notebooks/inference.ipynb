{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = Path(\"../data/raw/\")\n",
    "PROCESSED_PATH = Path(\"../data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_user_ids(data_path):\n",
    "    files = list(data_path.glob(\"HR_*.csv\"))\n",
    "    ids = [\n",
    "        re.search(r\"HR_(\\d+).csv\", f.name).group(1)\n",
    "        for f in files\n",
    "        if re.search(r\"HR_(\\d+).csv\", f.name)\n",
    "    ]\n",
    "    return sorted(list(set(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hr_data(file_path):\n",
    "    \"\"\"\n",
    "    STEP 1: CLEANING (Per User)\n",
    "    - Fix timestamps, remove duplicates, filter impossible values.\n",
    "    - NO feature engineering here.\n",
    "    \"\"\"\n",
    "    # 1. Load & Sort\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"HRTIME\"] = pd.to_datetime(df[\"HRTIME\"])\n",
    "    df = df.sort_values(\"HRTIME\")\n",
    "\n",
    "    # 2. Clean (Drop NaNs, Filter Outliers 40-180bpm)\n",
    "    df = df.dropna(subset=[\"HR\"])\n",
    "    df = df[(df[\"HR\"] > 40) & (df[\"HR\"] < 180)]\n",
    "\n",
    "    # 3. Handle Duplicates (merged the rows into one single row by calculating the average.)\n",
    "    df = df.groupby(\"HRTIME\", as_index=False)[\"HR\"].mean()\n",
    "    df = df.sort_values(\"HRTIME\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_hr_features(df):\n",
    "    \"\"\"\n",
    "    STEP 2: ENGINEERING (Per User)\n",
    "    - Rolling Windows & Expanding Baselines.\n",
    "    - Requires continuous time series (cannot be done after split).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set index for time-aware rolling\n",
    "    df = df.set_index(\"HRTIME\").sort_index()\n",
    "\n",
    "    # Rolling Features (Short-term State - 5 min window)\n",
    "    # min_periods=30 ensures we have at least 30 seconds of data\n",
    "    df[\"mean_hr_5min\"] = df[\"HR\"].rolling(\"5min\", min_periods=30).mean()\n",
    "    df[\"hr_volatility_5min\"] = df[\"HR\"].rolling(\"5min\", min_periods=30).std()\n",
    "\n",
    "    # --- Cumulative Features (Long-term Baseline) ---\n",
    "    df[\"hr_mean_total\"] = df[\"HR\"].expanding(min_periods=30).mean()\n",
    "    df[\"hr_std_total\"] = df[\"HR\"].expanding(min_periods=30).std()\n",
    "    df[\"hr_std_total\"] = df[\"hr_std_total\"].fillna(1)\n",
    "\n",
    "    # --- Z-Score (Standardized Severity) ---\n",
    "    df[\"hr_zscore\"] = (df[\"HR\"] - df[\"hr_mean_total\"]) / (df[\"hr_std_total\"] + 0.1)\n",
    "\n",
    "    # Jumpiness (Short Term Variability)\n",
    "    time_gap = df.index.to_series().diff().dt.total_seconds()\n",
    "    df[\"hr_diff\"] = df[\"HR\"].diff().abs()\n",
    "    df.loc[time_gap > 5, \"hr_diff\"] = np.nan  # Only diff if gap < 5s\n",
    "\n",
    "    df[\"hr_jumpiness_5min\"] = np.sqrt((df[\"hr_diff\"] ** 2).rolling(\"5min\", min_periods=30).mean())\n",
    "\n",
    "    # Stress Ratio (Coefficient of Variation)\n",
    "    df[\"stress_cv\"] = df[\"hr_volatility_5min\"] / (df[\"mean_hr_5min\"] + 0.1)\n",
    "\n",
    "    # Final Clean: Remove Warm-up NaNs\n",
    "    df = df.dropna(subset=[\"mean_hr_5min\"])\n",
    "\n",
    "    # Drop bad dates (Year 2000 bug)\n",
    "    df = df[df.index.year > 2021].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sleep_data(file_path):\n",
    "    \"\"\"\n",
    "    CLEANING (Per User)\n",
    "    - Responsibilities: Load, Clean NaNs/Duplicates, Filter logical durations.\n",
    "    - Returns: Clean DataFrame with valid timestamps and raw duration.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"START\"] = pd.to_datetime(df[\"START\"])\n",
    "    df[\"END\"] = pd.to_datetime(df[\"END\"])\n",
    "\n",
    "    # Remove missing values\n",
    "    df = df.dropna(subset=[\"START\", \"END\"])\n",
    "    # Clean Duplicates\n",
    "    df = df.drop_duplicates(subset=[\"START\", \"END\"])\n",
    "\n",
    "    # Calculate Duration (Needed for filtering)\n",
    "    df[\"duration_hours\"] = (df[\"END\"] - df[\"START\"]).dt.total_seconds() / 3600\n",
    "\n",
    "    # 3. Filter Logical Duration (15 min to 36 hours)\n",
    "    df = df[(df[\"duration_hours\"] > 0.25) & (df[\"duration_hours\"] < 36)]\n",
    "\n",
    "    # 4. Sort (Critical for rolling calculations later)\n",
    "    df = df.sort_values(\"END\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_sleep_features(df):\n",
    "    \"\"\"\n",
    "    ENGINEERING (Per User)\n",
    "    - Responsibilities: Calculate Sleep Debt and Cumulative Debt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rolling Debt Logic\n",
    "    df[\"sleep_debt\"] = 8.0 - df[\"duration_hours\"]\n",
    "\n",
    "    # Cumulative Debt (Rolling sum over last 3 sessions)\n",
    "    # Group by a dummy key because this function runs per-user\n",
    "    df[\"_temp_group\"] = 1\n",
    "    df[\"cum_sleep_debt\"] = (\n",
    "        df.groupby(\"_temp_group\")[\"sleep_debt\"]\n",
    "        .rolling(3, min_periods=1)\n",
    "        .sum()\n",
    "        .reset_index(0, drop=True)\n",
    "    )\n",
    "    return df[[\"START\", \"END\", \"duration_hours\", \"cum_sleep_debt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_continuous_features(uid, raw_path):\n",
    "    \"\"\"\n",
    "    Aggregates HR and Sleep data for a user regardless of PVT existence.\n",
    "    Anchor: Heart Rate timestamps.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Define Paths\n",
    "        hr_file = raw_path / f\"HR_{uid}.csv\"\n",
    "        sleep_file = raw_path / f\"sleep_{uid}.csv\"\n",
    "\n",
    "        if not hr_file.exists() or not sleep_file.exists():\n",
    "            return None\n",
    "\n",
    "        # --- 2. HR PROCESSING ---\n",
    "        df_hr_clean = clean_hr_data(hr_file)\n",
    "        df_hr_eng = engineer_hr_features(df_hr_clean)\n",
    "        if df_hr_eng is None:\n",
    "            return None\n",
    "\n",
    "        # --- 3. SLEEP PROCESSING ---\n",
    "        df_sleep_clean = clean_sleep_data(sleep_file)\n",
    "        df_sleep_eng = engineer_sleep_features(df_sleep_clean)\n",
    "\n",
    "        # Rename for merging\n",
    "        df_sleep_merge = df_sleep_eng.rename(columns={\"END\": \"last_sleep_end\"})\n",
    "\n",
    "        # --- 4. MERGE (HR as Anchor) ---\n",
    "        # We merge sleep data onto HR data based on the HRTIME\n",
    "        df_final = pd.merge_asof(\n",
    "            df_hr_eng.sort_values(\"HRTIME\"),\n",
    "            df_sleep_merge.sort_values(\"last_sleep_end\"),\n",
    "            left_on=\"HRTIME\",\n",
    "            right_on=\"last_sleep_end\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "\n",
    "        # --- 5. CALCULATE TIME-BASED FEATURES ---\n",
    "        # Hours since last sleep\n",
    "        df_final[\"hours_awake\"] = (\n",
    "            df_final[\"HRTIME\"] - df_final[\"last_sleep_end\"]\n",
    "        ).dt.total_seconds() / 3600\n",
    "\n",
    "        # Circadian Phase (based on current HR time)\n",
    "        hr_hour = df_final[\"HRTIME\"].dt.hour + (df_final[\"HRTIME\"].dt.minute / 60)\n",
    "        df_final[\"circadian_sin\"] = np.sin(2 * np.pi * hr_hour / 24)\n",
    "        df_final[\"circadian_cos\"] = np.cos(2 * np.pi * hr_hour / 24)\n",
    "\n",
    "        # Sleep Inertia\n",
    "        df_final[\"sleep_inertia_idx\"] = 1 / (df_final[\"hours_awake\"] + 0.1)\n",
    "\n",
    "        # --- 6. CLEANUP ---\n",
    "        # Add user_id and drop rows where we don't have sleep history\n",
    "        df_final[\"user_id\"] = uid\n",
    "        df_final = df_final.dropna(subset=[\"hours_awake\", \"cum_sleep_debt\"])\n",
    "\n",
    "        # FIX 1: Ensure HRTIME is the index and sorted before resampling\n",
    "        df_final = df_final.set_index(\"HRTIME\").sort_index()\n",
    "        df_final = df_final.resample(\"1min\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "        df_final[\"user_id\"] = uid\n",
    "\n",
    "        return df_final.dropna()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {uid}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 users.\n"
     ]
    }
   ],
   "source": [
    "all_continuous_data = []\n",
    "user_ids = get_all_user_ids(RAW_DATA_PATH)\n",
    "print(f\"Found {len(user_ids)} users.\")\n",
    "for uid in user_ids:\n",
    "    user_continuous = process_continuous_features(uid, RAW_DATA_PATH)\n",
    "    if user_continuous is not None:\n",
    "        all_continuous_data.append(user_continuous)\n",
    "\n",
    "df_continuous_total = pd.concat(all_continuous_data, ignore_index=True)\n",
    "df_continuous_total.to_parquet(PROCESSED_PATH / \"continuous_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               HRTIME          HR  mean_hr_5min  hr_volatility_5min  \\\n",
      "0 2023-04-14 19:44:00   88.631579     87.776873            5.593524   \n",
      "1 2023-04-14 19:45:00   91.315789     87.911509            4.714402   \n",
      "2 2023-04-14 19:46:00  118.098039     95.160720           12.303997   \n",
      "3 2023-04-14 19:47:00  137.421053    104.716552           19.269933   \n",
      "4 2023-04-14 19:48:00  111.687500    110.122351           19.809114   \n",
      "\n",
      "   hr_mean_total  hr_std_total  hr_zscore   hr_diff  hr_jumpiness_5min  \\\n",
      "0     103.128852     12.086594  -1.189605  1.222222           1.973392   \n",
      "1     103.119211     12.088983  -0.968380  1.388889           1.714790   \n",
      "2     103.123081     12.090308   1.228369  1.470588           1.674470   \n",
      "3     103.148566     12.117112   2.805460  1.324324           1.620771   \n",
      "4     103.168622     12.136320   0.696196  1.500000           1.573572   \n",
      "\n",
      "   stress_cv  duration_hours  cum_sleep_debt  hours_awake  circadian_sin  \\\n",
      "0   0.063657        2.766667        5.233333     2.742076      -0.898794   \n",
      "1   0.053545        2.766667        5.233333     2.760132      -0.896873   \n",
      "2   0.128762        2.766667        5.233333     2.774842      -0.894934   \n",
      "3   0.183574        2.766667        5.233333     2.790980      -0.892979   \n",
      "4   0.179798        2.766667        5.233333     2.807934      -0.891007   \n",
      "\n",
      "   circadian_cos  sleep_inertia_idx user_id START last_sleep_end  \n",
      "0       0.438371           0.351856    3369   NaT            NaT  \n",
      "1       0.442289           0.349635    3369   NaT            NaT  \n",
      "2       0.446198           0.347846    3369   NaT            NaT  \n",
      "3       0.450098           0.345905    3369   NaT            NaT  \n",
      "4       0.453990           0.343888    3369   NaT            NaT  \n",
      "224709\n"
     ]
    }
   ],
   "source": [
    "print(df_continuous_total.head())\n",
    "print(len(df_continuous_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                HRTIME user_id  fatigue_probability  fatigue_event_alert  \\\n",
      "0  2023-04-14 19:44:00    3369             0.388276                    1   \n",
      "1  2023-04-14 19:45:00    3369             0.388276                    1   \n",
      "2  2023-04-14 19:46:00    3369             0.378783                    1   \n",
      "3  2023-04-14 19:47:00    3369             0.420439                    1   \n",
      "4  2023-04-14 19:48:00    3369             0.381330                    1   \n",
      "\n",
      "     status  \n",
      "0  Fatigued  \n",
      "1  Fatigued  \n",
      "2  Fatigued  \n",
      "3  Fatigued  \n",
      "4  Fatigued  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_parquet(\n",
    "    \"/Users/amyshih/Desktop/python/fatigue_prediction/data/predictions/fatigue_predictions.parquet\"\n",
    ")\n",
    "print(results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract\n",
    "2. Transform\n",
    "3. Split\n",
    "4. Outlier Removal only from the Training set and then scaling\n",
    "5. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONS\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "RAW_DATA_PATH = Path(\"../data/raw/\")\n",
    "OFFSETS = [0, 1, 2, 3, 4]  # Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Definitions\n",
    "# Columns that go into the model\n",
    "FEATURE_COLS = [\n",
    "    \"mean_hr_5min\",\n",
    "    \"hr_volatility_5min\",\n",
    "    \"hr_jumpiness_5min\",\n",
    "    \"hr_mean_total\",\n",
    "    \"hr_std_total\",\n",
    "    \"stress_cv\",\n",
    "    \"hours_awake\",\n",
    "    \"cum_sleep_debt\",\n",
    "    \"sleep_inertia_idx\",\n",
    "    \"circadian_sin\",\n",
    "    \"circadian_cos\",\n",
    "    \"hr_zscore\",\n",
    "    \"user_id\",  # Needed for splitting, will be dropped before training\n",
    "]\n",
    "\n",
    "# Columns that require scaling (Subset of above)\n",
    "SCALE_COLS = [\n",
    "    \"mean_hr_5min\",\n",
    "    \"hr_volatility_5min\",\n",
    "    \"hr_jumpiness_5min\",\n",
    "    \"hr_mean_total\",\n",
    "    \"hr_std_total\",\n",
    "    \"stress_cv\",\n",
    "    \"sleep_inertia_idx\",\n",
    "    \"hours_awake\",\n",
    "    \"cum_sleep_debt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "TARGET_COL = \"is_fatigued\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "SAMPLE_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "PROCESSED_PATH = Path(\"../data/processed/\")\n",
    "INPUTS_TRAIN_FILE = PROCESSED_PATH / \"inputs_train.parquet\"\n",
    "INPUTS_TEST_FILE = PROCESSED_PATH / \"inputs_test.parquet\"\n",
    "TARGETS_TRAIN_FILE = PROCESSED_PATH / \"targets_train.parquet\"\n",
    "TARGETS_TEST_FILE = PROCESSED_PATH / \"targets_test.parquet\"\n",
    "INPUTS_SAMPLE_FILE = PROCESSED_PATH / \"inputs_sample.parquet\"\n",
    "TARGETS_SAMPLE_FILE = PROCESSED_PATH / \"targets_sample.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_user_ids(data_path):\n",
    "    files = list(data_path.glob(\"HR_*.csv\"))\n",
    "    ids = [\n",
    "        re.search(r\"HR_(\\d+).csv\", f.name).group(1)\n",
    "        for f in files\n",
    "        if re.search(r\"HR_(\\d+).csv\", f.name)\n",
    "    ]\n",
    "    return sorted(list(set(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING (Per User) and ENGINEERING (Per User) for heart rate, pvt, and sleep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hr_data(file_path):\n",
    "    \"\"\"\n",
    "    STEP 1: CLEANING (Per User)\n",
    "    - Fix timestamps, remove duplicates, filter impossible values.\n",
    "    - NO feature engineering here.\n",
    "    \"\"\"\n",
    "    # 1. Load & Sort\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"HRTIME\"] = pd.to_datetime(df[\"HRTIME\"])\n",
    "    df = df.sort_values(\"HRTIME\")\n",
    "\n",
    "    # 2. Clean (Drop NaNs, Filter Outliers 40-180bpm)\n",
    "    df = df.dropna(subset=[\"HR\"])\n",
    "    df = df[(df[\"HR\"] > 40) & (df[\"HR\"] < 180)]\n",
    "\n",
    "    # 3. Handle Duplicates (merged the rows into one single row by calculating the average.)\n",
    "    df = df.groupby(\"HRTIME\", as_index=False)[\"HR\"].mean()\n",
    "    df = df.sort_values(\"HRTIME\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_hr_features(df):\n",
    "    \"\"\"\n",
    "    STEP 2: ENGINEERING (Per User)\n",
    "    - Rolling Windows & Expanding Baselines.\n",
    "    - Requires continuous time series (cannot be done after split).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set index for time-aware rolling\n",
    "    df = df.set_index(\"HRTIME\").sort_index()\n",
    "\n",
    "    # Rolling Features (Short-term State - 5 min window)\n",
    "    # min_periods=30 ensures we have at least 30 seconds of data\n",
    "    df[\"mean_hr_5min\"] = df[\"HR\"].rolling(\"5min\", min_periods=30).mean()\n",
    "    df[\"hr_volatility_5min\"] = df[\"HR\"].rolling(\"5min\", min_periods=30).std()\n",
    "\n",
    "    # --- Cumulative Features (Long-term Baseline) ---\n",
    "    df[\"hr_mean_total\"] = df[\"HR\"].expanding(min_periods=30).mean()\n",
    "    df[\"hr_std_total\"] = df[\"HR\"].expanding(min_periods=30).std()\n",
    "    df[\"hr_std_total\"] = df[\"hr_std_total\"].fillna(1)\n",
    "\n",
    "    # --- Z-Score (Standardized Severity) ---\n",
    "    df[\"hr_zscore\"] = (df[\"HR\"] - df[\"hr_mean_total\"]) / (df[\"hr_std_total\"] + 0.1)\n",
    "\n",
    "    # Jumpiness (Short Term Variability)\n",
    "    time_gap = df.index.to_series().diff().dt.total_seconds()\n",
    "    df[\"hr_diff\"] = df[\"HR\"].diff().abs()\n",
    "    df.loc[time_gap > 5, \"hr_diff\"] = np.nan  # Only diff if gap < 5s\n",
    "\n",
    "    df[\"hr_jumpiness_5min\"] = np.sqrt((df[\"hr_diff\"] ** 2).rolling(\"5min\", min_periods=30).mean())\n",
    "\n",
    "    # Stress Ratio (Coefficient of Variation)\n",
    "    df[\"stress_cv\"] = df[\"hr_volatility_5min\"] / (df[\"mean_hr_5min\"] + 0.1)\n",
    "\n",
    "    # Final Clean: Remove Warm-up NaNs\n",
    "    df = df.dropna(subset=[\"mean_hr_5min\"])\n",
    "\n",
    "    # Drop bad dates (Year 2000 bug)\n",
    "    df = df[df.index.year > 2021].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sleep_data(file_path):\n",
    "    \"\"\"\n",
    "    CLEANING (Per User)\n",
    "    - Responsibilities: Load, Clean NaNs/Duplicates, Filter logical durations.\n",
    "    - Returns: Clean DataFrame with valid timestamps and raw duration.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"START\"] = pd.to_datetime(df[\"START\"])\n",
    "    df[\"END\"] = pd.to_datetime(df[\"END\"])\n",
    "\n",
    "    # Remove missing values\n",
    "    df = df.dropna(subset=[\"START\", \"END\"])\n",
    "    # Clean Duplicates\n",
    "    df = df.drop_duplicates(subset=[\"START\", \"END\"])\n",
    "\n",
    "    # Calculate Duration (Needed for filtering)\n",
    "    df[\"duration_hours\"] = (df[\"END\"] - df[\"START\"]).dt.total_seconds() / 3600\n",
    "\n",
    "    # 3. Filter Logical Duration (15 min to 36 hours)\n",
    "    df = df[(df[\"duration_hours\"] > 0.25) & (df[\"duration_hours\"] < 36)]\n",
    "\n",
    "    # 4. Sort (Critical for rolling calculations later)\n",
    "    df = df.sort_values(\"END\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_sleep_features(df):\n",
    "    \"\"\"\n",
    "    ENGINEERING (Per User)\n",
    "    - Responsibilities: Calculate Sleep Debt and Cumulative Debt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rolling Debt Logic\n",
    "    df[\"sleep_debt\"] = 8.0 - df[\"duration_hours\"]\n",
    "\n",
    "    # Cumulative Debt (Rolling sum over last 3 sessions)\n",
    "    # Group by a dummy key because this function runs per-user\n",
    "    df[\"_temp_group\"] = 1\n",
    "    df[\"cum_sleep_debt\"] = (\n",
    "        df.groupby(\"_temp_group\")[\"sleep_debt\"]\n",
    "        .rolling(3, min_periods=1)\n",
    "        .sum()\n",
    "        .reset_index(0, drop=True)\n",
    "    )\n",
    "    return df[[\"START\", \"END\", \"duration_hours\", \"cum_sleep_debt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pvt_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"TESTSTART\"] = pd.to_datetime(df[\"TESTSTART\"])\n",
    "    df = df[df[\"TAPTIME\"] > 100]  # Filter Artifacts (False starts < 100ms)\n",
    "    # Aggregate per Test\n",
    "    df = df.groupby([\"TESTID\", \"TESTSTART\"]).agg({\"TAPTIME\": \"mean\"}).reset_index()\n",
    "    df.rename(columns={\"TAPTIME\": \"pvt_mean_rt\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classes(rt):\n",
    "    \"\"\"\n",
    "    Map PVT reaction time to continuous fatigue risk [0, 1]\n",
    "    \"\"\"\n",
    "    if rt < 300:\n",
    "        return 0.0\n",
    "    elif rt < 400:\n",
    "        # Linear interpolation between 300â€“400 ms\n",
    "        return (rt - 300) / 100.0\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_user_pipeline(uid, raw_path, offsets):\n",
    "    \"\"\"\n",
    "    The Master Function.\n",
    "    Coordinates loading, cleaning, engineering, and merging for ONE user.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Define Paths\n",
    "        hr_file = raw_path / f\"HR_{uid}.csv\"\n",
    "        pvt_file = raw_path / f\"pvt_{uid}.csv\"\n",
    "        sleep_file = raw_path / f\"sleep_{uid}.csv\"\n",
    "\n",
    "        # 2. Check Existence\n",
    "        if not (hr_file.exists() and pvt_file.exists() and sleep_file.exists()):\n",
    "            return None\n",
    "\n",
    "        # --- HR PROCESSING ---\n",
    "        df_hr_clean = clean_hr_data(hr_file)\n",
    "        if df_hr_clean is None:\n",
    "            return None\n",
    "\n",
    "        df_hr_eng = engineer_hr_features(df_hr_clean)\n",
    "        if df_hr_eng is None:\n",
    "            return None\n",
    "\n",
    "        # --- PVT PROCESSING ---\n",
    "        df_pvt = process_pvt_data(pvt_file)\n",
    "        if df_pvt is None:\n",
    "            return None\n",
    "\n",
    "        # --- SLEEP PROCESSING (Split) ---\n",
    "        df_sleep_clean = clean_sleep_data(sleep_file)\n",
    "        if df_sleep_clean is None:\n",
    "            return None\n",
    "\n",
    "        df_sleep_eng = engineer_sleep_features(df_sleep_clean)\n",
    "        if df_sleep_eng is None:\n",
    "            return None\n",
    "\n",
    "        # 4. Augmentation (Merge HR + PVT)\n",
    "        augmented_dfs = []\n",
    "        for offset in offsets:\n",
    "            targets_shifted = df_pvt.copy()\n",
    "            targets_shifted[\"MATCH_TIME\"] = targets_shifted[\"TESTSTART\"] - pd.Timedelta(\n",
    "                minutes=offset\n",
    "            )\n",
    "\n",
    "            merged = pd.merge_asof(\n",
    "                targets_shifted.sort_values(\"MATCH_TIME\"),\n",
    "                df_hr_eng.sort_values(\"HRTIME\"),\n",
    "                left_on=\"MATCH_TIME\",\n",
    "                right_on=\"HRTIME\",\n",
    "                direction=\"backward\",\n",
    "                tolerance=pd.Timedelta(\"10 minutes\"),\n",
    "            )\n",
    "            augmented_dfs.append(merged)\n",
    "\n",
    "        df_aug = pd.concat(augmented_dfs)\n",
    "        df_aug = df_aug.dropna(subset=[\"mean_hr_5min\"])\n",
    "\n",
    "        # 5. Merge Sleep\n",
    "        df_sleep_merge = df_sleep_eng.rename(columns={\"END\": \"last_sleep_end\"})\n",
    "        df_final = pd.merge_asof(\n",
    "            df_aug.sort_values(\"TESTSTART\"),\n",
    "            df_sleep_merge.sort_values(\"last_sleep_end\"),\n",
    "            left_on=\"TESTSTART\",\n",
    "            right_on=\"last_sleep_end\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "\n",
    "        # 6. Context Features & Imputation\n",
    "        df_final[\"hours_awake\"] = (\n",
    "            df_final[\"TESTSTART\"] - df_final[\"last_sleep_end\"]\n",
    "        ).dt.total_seconds() / 3600\n",
    "\n",
    "        # Impute Missing Sleep\n",
    "        if df_final[\"hours_awake\"].isna().any():\n",
    "            user_median = df_final[\"hours_awake\"].median()\n",
    "            fill_val = user_median if pd.notna(user_median) else 8.0\n",
    "            df_final[\"hours_awake\"] = df_final[\"hours_awake\"].fillna(fill_val)\n",
    "            if \"cum_sleep_debt\" in df_final.columns:\n",
    "                df_final[\"cum_sleep_debt\"] = df_final[\"cum_sleep_debt\"].fillna(0.0)\n",
    "\n",
    "        # Recalculate Inertia & Circadian\n",
    "        df_final[\"sleep_inertia_idx\"] = 1 / (df_final[\"hours_awake\"] + 0.1)\n",
    "        test_hour = df_final[\"TESTSTART\"].dt.hour + (df_final[\"TESTSTART\"].dt.minute / 60)\n",
    "        df_final[\"circadian_sin\"] = np.sin(2 * np.pi * test_hour / 24)\n",
    "        df_final[\"circadian_cos\"] = np.cos(2 * np.pi * test_hour / 24)\n",
    "\n",
    "        # Sleep Inertia\n",
    "        df_final[\"sleep_inertia_idx\"] = 1 / (df_final[\"hours_awake\"] + 0.1)\n",
    "\n",
    "        # 7. Final Clean\n",
    "        df_final_user = df_final.dropna(subset=[\"hours_awake\", \"mean_hr_5min\"])\n",
    "        df_final_user[\"user_id\"] = uid\n",
    "\n",
    "        return df_final_user\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {uid}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2099 entries, 0 to 2098\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          2099 non-null   int64 \n",
      " 1   TESTID      2099 non-null   int64 \n",
      " 2   TESTSTART   2099 non-null   object\n",
      " 3   TRIALID     2099 non-null   int64 \n",
      " 4   TRIALNAME   2099 non-null   object\n",
      " 5   TRIALSTART  2099 non-null   object\n",
      " 6   TAPTIME     2099 non-null   int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 114.9+ KB\n"
     ]
    }
   ],
   "source": [
    "pvt_file = RAW_DATA_PATH / f\"pvt_{3369}.csv\"\n",
    "df = pd.read_csv(pvt_file)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DATASET LEVEL PIPELINE ---\n",
    "\n",
    "\n",
    "def split_and_scale_dataset(df, feature_cols, scale_cols, target_func):\n",
    "    \"\"\"\n",
    "    Takes the raw combined dataframe and returns ready-to-train arrays.\n",
    "    Performs: Target Creation -> Feature Select -> Split -> Scale\n",
    "    \"\"\"\n",
    "    # 1. Apply Target Logic\n",
    "    df[\"fatigue_score\"] = df[\"pvt_mean_rt\"].apply(target_func)\n",
    "\n",
    "    # 2. Select Features\n",
    "    inputs = df[feature_cols].copy()\n",
    "    targets = df[[\"fatigue_score\"]].copy()\n",
    "\n",
    "    # 3. Split (GroupShuffleSplit)\n",
    "    # This respects User independence\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    train_idx, test_idx = next(splitter.split(inputs, targets, groups=inputs[\"user_id\"]))\n",
    "\n",
    "    X_train = inputs.iloc[train_idx].copy()\n",
    "    X_test = inputs.iloc[test_idx].copy()\n",
    "    y_train = targets.iloc[train_idx].copy()\n",
    "    y_test = targets.iloc[test_idx].copy()\n",
    "\n",
    "    # 4. Scale (Fit on Train, Transform Test)\n",
    "    print(f\"Scaling {len(scale_cols)} columns...\")\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    X_train[scale_cols] = scaler.fit_transform(X_train[scale_cols])\n",
    "    X_test[scale_cols] = scaler.transform(X_test[scale_cols])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 users.\n",
      "Starting Phase 1: Per-User Processing...\n",
      "User 3369: Added 125 rows.\n",
      "User 3372: Added 0 rows.\n",
      "User 3394: Added 5 rows.\n",
      "User 3566: Added 25 rows.\n",
      "User 38213: Added 10 rows.\n",
      "User 4692: Added 20 rows.\n",
      "User 5135: Added 5 rows.\n",
      "User 54720: Added 50 rows.\n",
      "User 5876: Added 20 rows.\n",
      "User 6073: Added 0 rows.\n",
      "User 6321: Added 0 rows.\n",
      "User 6366: Added 0 rows.\n",
      "User 6391: Added 25 rows.\n",
      "User 6561: Added 0 rows.\n",
      "User 6724: Added 5 rows.\n",
      "User 7591: Added 85 rows.\n",
      "User 7982680: Added 40 rows.\n",
      "User 7982818: Added 25 rows.\n",
      "User 8068: Added 0 rows.\n",
      "User 8340242: Added 35 rows.\n",
      "User 8394: Added 26 rows.\n",
      "User 8489754: Added 96 rows.\n",
      "User 8489783: Added 70 rows.\n",
      "User 8489813: Added 55 rows.\n",
      "User 8490189: Added 55 rows.\n",
      "User 8562291: Added 40 rows.\n",
      "User 8562423: Added 31 rows.\n",
      "User 8562476: Added 10 rows.\n",
      "User 8632: Added 480 rows.\n",
      "Total Extraction: (1338, 23)\n"
     ]
    }
   ],
   "source": [
    "# --- EXTRACT ---\n",
    "all_user_data = []\n",
    "user_ids = get_all_user_ids(RAW_DATA_PATH)\n",
    "print(f\"Found {len(user_ids)} users.\")\n",
    "\n",
    "print(\"Starting Phase 1: Per-User Processing...\")\n",
    "for uid in user_ids:\n",
    "    user_df = process_single_user_pipeline(uid, RAW_DATA_PATH, OFFSETS)\n",
    "\n",
    "    if user_df is not None:\n",
    "        all_user_data.append(user_df)\n",
    "        print(f\"User {uid}: Added {len(user_df)} rows.\")\n",
    "    else:\n",
    "        print(f\"User {uid}: Skipped.\")\n",
    "\n",
    "# Aggregate\n",
    "df_total = pd.concat(all_user_data, ignore_index=True)\n",
    "print(f\"Total Extraction: {df_total.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.to_csv(PROCESSED_PATH / \"full_dataset_preprocess.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 2: Splitting and Scaling...\n",
      "Scaling 9 columns...\n"
     ]
    }
   ],
   "source": [
    "# B. Phase 2: Dataset-Level Processing (Split & Scale)\n",
    "print(\"Starting Phase 2: Splitting and Scaling...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_and_scale_dataset(\n",
    "    df=df_total,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    scale_cols=SCALE_COLS,\n",
    "    target_func=create_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (1032, 13), Test Shape: (306, 13)\n",
      "Data successfully saved to: ../data/processed\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Save Artifacts\n",
    "print(f\"Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")\n",
    "\n",
    "# Create Debug Sample (Aligned)\n",
    "X_sample = X_train.sample(n=min(SAMPLE_SIZE, len(X_train)), random_state=RANDOM_STATE)\n",
    "y_sample = y_train.loc[X_sample.index]\n",
    "\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_train.to_parquet(INPUTS_TRAIN_FILE)\n",
    "X_test.to_parquet(INPUTS_TEST_FILE)\n",
    "y_train.to_parquet(TARGETS_TRAIN_FILE)\n",
    "y_test.to_parquet(TARGETS_TEST_FILE)\n",
    "X_sample.to_parquet(INPUTS_SAMPLE_FILE)\n",
    "y_sample.to_parquet(TARGETS_SAMPLE_FILE)\n",
    "\n",
    "print(\"Data successfully saved to:\", PROCESSED_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
